@online{ARKitBlendShape,
  title = {{{ARKit Blend Shape Location}}},
  url = {https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation},
  urldate = {2023-06-03},
  abstract = {Identifiers for specific facial features, for use with coefficients describing the relative movements of those features.},
  langid = {english},
  organization = {{Apple Developer Documentation}},
  file = {C:\Users\daohi\Zotero\storage\VTPXMRWQ\blendshapelocation.html}
}

@article{baoSurveyImageBasedTechniques2018,
  title = {A {{Survey}} of {{Image-Based Techniques}} for {{Hair Modeling}}},
  author = {Bao, Yongtang and Qi, Yue},
  date = {2018},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {6},
  pages = {18670--18684},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2818795},
  url = {http://ieeexplore.ieee.org/document/8323371/},
  urldate = {2023-11-23},
  abstract = {With the tremendous performance increase of today’s graphics technologies, visual details of digital humans in games, online virtual worlds, and virtual reality applications are becoming significantly more demanding. Hair is a vital component of a person’s identity and can provide strong cues about age, background, and even personality. More and more researchers focus on hair modeling in the fields of computer graphics and virtual reality. Traditional methods are physics-based simulation by setting different parameters. The computation is expensive, and the constructing process is non-intuitive, difficult to control. Conversely, image-based methods have the advantages of fast modeling and high fidelity. This paper surveys the state of the art in the major topics of image-based techniques for hair modeling, including single-view hair modeling, static hair modeling from multiple images, video-based dynamic hair modeling, and the editing and reusing of hair modeling results. We first summarize the single-view approaches, which can be divided into the orientation-field and data-driven-based methods. The static methods from multiple images and dynamic methods are then reviewed in Sections III and IV. In Section V, we also review the editing and reusing of hair modeling results. The future development trends and challenges of image-based methods are proposed in the end.},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\JIHVWZ7E\Bao and Qi - 2018 - A Survey of Image-Based Techniques for Hair Modeli.pdf}
}

@inproceedings{blanzMorphableModelSynthesis1999,
  title = {A Morphable Model for the Synthesis of {{3D}} Faces},
  booktitle = {Proceedings of the 26th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '99},
  author = {Blanz, Volker and Vetter, Thomas},
  date = {1999},
  pages = {187--194},
  publisher = {{ACM Press}},
  location = {{Not Known}},
  doi = {10.1145/311535.311556},
  url = {http://portal.acm.org/citation.cfm?doid=311535.311556},
  urldate = {2023-06-05},
  abstract = {In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an “unlikely” appearance.},
  eventtitle = {The 26th Annual Conference},
  isbn = {978-0-201-48560-8},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\FKZBZR7R\Blanz and Vetter - 1999 - A morphable model for the synthesis of 3D faces.pdf}
}

@article{caoFaceWarehouse3DFacial2014,
  title = {{{FaceWarehouse}}: {{A 3D Facial Expression Database}} for {{Visual Computing}}},
  shorttitle = {{{FaceWarehouse}}},
  author = {Cao, Chen and Weng, Yanlin and Zhou, Shun and Tong, Yiying and Zhou, Kun},
  date = {2014-03},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {3},
  pages = {413--425},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2013.249},
  abstract = {We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.},
  eventtitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  keywords = {Color,Computational modeling,Databases,FA,Face,face database,Face modeling,facial animation,mesh deformation,Mouth,RGBD camera,Solid modeling,Three-dimensional displays},
  file = {C:\Users\daohi\Zotero\storage\5CI85T5V\6654137.html}
}

@inproceedings{danecekEMOCAEmotionDriven2022,
  title = {{{EMOCA}}: {{Emotion Driven Monocular Face Capture}} and {{Animation}}},
  shorttitle = {{{EMOCA}}},
  author = {Daněček, Radek and Black, Michael J. and Bolkart, Timo},
  date = {2022},
  pages = {20311--20322},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Danecek_EMOCA_Emotion_Driven_Monocular_Face_Capture_and_Animation_CVPR_2022_paper.html},
  urldate = {2023-06-13},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\7JG83MBC\Daněček et al. - 2022 - EMOCA Emotion Driven Monocular Face Capture and A.pdf}
}

@online{dengRetinaFaceSinglestageDense2019,
  title = {{{RetinaFace}}: {{Single-stage Dense Face Localisation}} in the {{Wild}}},
  shorttitle = {{{RetinaFace}}},
  author = {Deng, Jiankang and Guo, Jia and Zhou, Yuxiang and Yu, Jinke and Kotsia, Irene and Zafeiriou, Stefanos},
  date = {2019-05-04},
  eprint = {1905.00641},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.00641},
  url = {http://arxiv.org/abs/1905.00641},
  urldate = {2023-11-23},
  abstract = {Though tremendous strides have been made in uncontrolled face detection, accurate and efficient face localisation in the wild remains an open challenge. This paper presents a robust single-stage face detector, named RetinaFace, which performs pixel-wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self-supervised multi-task learning. Specifically, We make contributions in the following five aspects: (1) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal. (2) We further add a self-supervised mesh decoder branch for predicting a pixel-wise 3D shape face information in parallel with the existing supervised branches. (3) On the WIDER FACE hard test set, RetinaFace outperforms the state of the art average precision (AP) by 1.1\% (achieving AP equal to 91.4\%). (4) On the IJB-C test set, RetinaFace enables state of the art methods (ArcFace) to improve their results in face verification (TAR=89.59\% for FAR=1e-6). (5) By employing light-weight backbone networks, RetinaFace can run real-time on a single CPU core for a VGA-resolution image. Extra annotations and code have been made available at: https://github.com/deepinsight/insightface/tree/master/RetinaFace.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\W7TAQQG7\\Deng et al. - 2019 - RetinaFace Single-stage Dense Face Localisation i.pdf;C\:\\Users\\daohi\\Zotero\\storage\\MXVBYBQQ\\1905.html}
}

@incollection{dengSubcenterArcFaceBoosting2020,
  title = {Sub-Center {{ArcFace}}: {{Boosting Face Recognition}} by {{Large-Scale Noisy Web Faces}}},
  shorttitle = {Sub-Center {{ArcFace}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Deng, Jiankang and Guo, Jia and Liu, Tongliang and Gong, Mingming and Zafeiriou, Stefanos},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  volume = {12356},
  pages = {741--757},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58621-8_43},
  url = {https://link.springer.com/10.1007/978-3-030-58621-8_43},
  urldate = {2023-11-22},
  abstract = {Margin-based deep face recognition methods (e.g. SphereFace, CosFace, and ArcFace) have achieved remarkable success in unconstrained face recognition. However, these methods are susceptible to the massive label noise in the training data and thus require laborious human effort to clean the datasets. In this paper, we relax the intra-class constraint of ArcFace to improve the robustness to label noise. More specifically, we design K sub-centers for each class and the training sample only needs to be close to any of the K positive sub-centers instead of the only one positive center. The proposed sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Extensive experiments confirm the robustness of sub-center ArcFace under massive real-world noise. After the model achieves enough discriminative power, we directly drop non-dominant sub-centers and high-confident noisy samples, which helps recapture intra-compactness, decrease the influence from noise, and achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. By taking advantage of the large-scale raw web faces (Celeb500K), sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT.},
  isbn = {978-3-030-58620-1 978-3-030-58621-8},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\5WK5PF2R\Deng et al. - 2020 - Sub-center ArcFace Boosting Face Recognition by L.pdf}
}

@article{fengLearningAnimatableDetailed2021,
  title = {Learning an Animatable Detailed {{3D}} Face Model from In-the-Wild Images},
  author = {Feng, Yao and Feng, Haiwen and Black, Michael J. and Bolkart, Timo},
  date = {2021-07-19},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {88:1--88:13},
  issn = {0730-0301},
  doi = {10.1145/3450626.3459936},
  url = {https://dl.acm.org/doi/10.1145/3450626.3459936},
  urldate = {2023-05-28},
  abstract = {While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://deca.is.tue.mpg.de.},
  keywords = {3D face reconstruction,DECA,detail disentanglement,detailed face model,facial animation},
  file = {C:\Users\daohi\Zotero\storage\DWMNFTN8\Feng et al. - 2021 - Learning an animatable detailed 3D face model from.pdf}
}

@software{fengPhotometricFLAMEFitting2023,
  title = {Photometric {{FLAME Fitting}}},
  author = {Feng, Haven},
  date = {2023-06-14T01:56:12Z},
  origdate = {2020-03-29T02:06:38Z},
  url = {https://github.com/HavenFeng/photometric_optimization},
  urldate = {2023-06-14},
  abstract = {Photometric optimization code for creating the FLAME texture space and other applications},
  keywords = {3d-face-reconstruction,3d-graphics,3d-model,3d-shapes,computer-vision,ffhq,flame-model,flame-texture,optimization,python,pytorch-implementation,pytorch3d,texture-maps,texture-space}
}

@inproceedings{galanakis3DMMRFConvolutionalRadiance2023,
  title = {{{3DMM-RF}}: {{Convolutional Radiance Fields}} for {{3D Face Modeling}}},
  shorttitle = {{{3DMM-RF}}},
  author = {Galanakis, Stathis and Gecer, Baris and Lattas, Alexandros and Zafeiriou, Stefanos},
  date = {2023},
  pages = {3536--3547},
  url = {https://openaccess.thecvf.com/content/WACV2023/html/Galanakis_3DMM-RF_Convolutional_Radiance_Fields_for_3D_Face_Modeling_WACV_2023_paper.html},
  urldate = {2023-06-13},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\WWC8ZBCA\Galanakis et al. - 2023 - 3DMM-RF Convolutional Radiance Fields for 3D Face.pdf}
}

@article{gecerFastGANFITGenerativeAdversarial2022,
  title = {Fast-{{GANFIT}}: {{Generative Adversarial Network}} for {{High Fidelity 3D Face Reconstruction}}},
  shorttitle = {Fast-{{GANFIT}}},
  author = {Gecer, Baris and Ploumpis, Stylianos and Kotsia, Irene and Zafeiriou, Stefanos},
  date = {2022-09},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {9},
  pages = {4879--4893},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3084524},
  abstract = {A lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of deep convolutional neural networks (DCNNs). In the recent works, the texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction is still not capable of modeling facial texture with high-frequency details. In this paper, we take a radically different approach and harness the power of generative adversarial networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful facial texture prior from a large-scale 3D texture dataset. Then, we revisit the original 3D Morphable Models (3DMMs) fitting making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. In order to be robust towards initialisation and expedite the fitting process, we propose a novel self-supervised regression based approach. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {3d face reconstruction,3D morphable models,3DMM,Cost function,Face recognition,fitting,Generative adversarial networks,high-quality texture,identity preservation,Image reconstruction,regression,Shape,Solid modeling,Three-dimensional displays},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\XFC8T8ET\\Gecer et al. - 2022 - Fast-GANFIT Generative Adversarial Network for Hi.pdf;C\:\\Users\\daohi\\Zotero\\storage\\F94G8JX3\\9442802.html}
}

@inproceedings{gerigMorphableFaceModels2018,
  title = {Morphable {{Face Models}} - {{An Open Framework}}},
  booktitle = {2018 13th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2018)},
  author = {Gerig, Thomas and Morel-Forster, Andreas and Blumer, Clemens and Egger, Bernhard and Luthi, Marcel and Schoenborn, Sandro and Vetter, Thomas},
  date = {2018-05},
  pages = {75--82},
  doi = {10.1109/FG.2018.00021},
  abstract = {In this paper, we present a novel open-source pipeline for face registration based on Gaussian processes as well as an application to face image analysis. Non-rigid registration of faces is significant for many applications in computer vision, such as the construction of 3D Morphable face models (3DMMs). Gaussian Process Morphable Models (GPMMs) unify a variety of non-rigid deformation models with B-splines and PCA models as examples. GPMM separate problem specific requirements from the registration algorithm by incorporating domain-specific adaptions as a prior model. The novelties of this paper are the following: (i) We present a strategy and modeling technique for face registration that considers symmetry, multi-scale and spatially-varying details. The registration is applied to neutral faces and facial expressions. (ii) We release an open-source software framework for registration model-building demonstrated on the publicly available BU3D-FE database. The released pipeline also contains an implementation of an Analysis-by-Synthesis model adaption of 2D face images, tested on the Multi-PIE and LFW database. This enables the community to reproduce, evaluate and compare the individual steps of registration to model-building and 3D/2D model fitting. (iii) Along with the framework release, we publish a new version of the Basel Face Model (BFM-2017) with an improved age distribution and an additional facial expression model.},
  eventtitle = {2018 13th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2018)},
  keywords = {Adaptation models,Basel Face Model,Basel face model newer,Deformable models,Face,Face Reconstruction,Gaussian Process Morphable Model,Gaussian processes,Kernel,Morphable Model,Registration,Shape,Strain},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\SK7X2DMM\\Gerig et al. - 2018 - Morphable Face Models - An Open Framework.pdf;C\:\\Users\\daohi\\Zotero\\storage\\L5PWR4SJ\\8373814.html}
}

@inproceedings{hongHeadNeRFRealTimeNeRFBased2022,
  title = {{{HeadNeRF}}: {{A Real-Time NeRF-Based Parametric Head Model}}},
  shorttitle = {{{HeadNeRF}}},
  author = {Hong, Yang and Peng, Bo and Xiao, Haiyao and Liu, Ligang and Zhang, Juyong},
  date = {2022},
  pages = {20374--20384},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.html},
  urldate = {2023-06-13},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\7CB9QI43\Hong et al. - 2022 - HeadNeRF A Real-Time NeRF-Based Parametric Head M.pdf}
}

@inproceedings{karrasStyleBasedGeneratorArchitecture2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  date = {2019},
  pages = {4401--4410},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html},
  urldate = {2023-06-14},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\daohi\Zotero\storage\5ZSVHCDS\Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf}
}

@book{kavanRealTimeSkin2003a,
  title = {Real {{Time Skin Deformation}} with {{Bones Blending}}},
  author = {Kavan, Ladislav and Žára, Jiří},
  date = {2003},
  publisher = {{UNION Agency}},
  url = {http://dspace5.zcu.cz/handle/11025/6166},
  urldate = {2023-06-01},
  abstract = {Skeletal animation with vertex blending is a popular method to model believable 3D characters. Its main advantage is its speed that allows real-time deformation even on low-level hardware. However it has some drawbacks as well: for some movements it produces non-natural postures like twisting elbows. We present a new method called bones blending that can be used as an alternative to vertex blending. Bones blending has a power to overcome the artifacts of vertex blending.},
  isbn = {978-80-903100-1-8},
  langid = {english},
  annotation = {Accepted: 2013-09-06T05:44:25Z},
  file = {C:\Users\daohi\Zotero\storage\JKISW8VB\Kavan and Žára - 2003 - Real Time Skin Deformation with Bones Blending.pdf}
}

@article{liLearningModelFacial2017,
  title = {Learning a Model of Facial Shape and Expression from {{4D}} Scans},
  author = {Li, Tianye and Bolkart, Timo and Black, Michael J. and Li, Hao and Romero, Javier},
  date = {2017-11-20},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {36},
  number = {6},
  pages = {194:1--194:17},
  issn = {0730-0301},
  doi = {10.1145/3130800.3130813},
  url = {https://dl.acm.org/doi/10.1145/3130800.3130813},
  urldate = {2023-06-05},
  abstract = {The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The pose and expression dependent articulations are learned from 4D face sequences in the D3DFACS dataset along with additional 4D sequences. We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).},
  keywords = {4D registration,blend skinning,face model,facial expression,FLAME,learning,mesh registration,shape},
  file = {C:\Users\daohi\Zotero\storage\V7JBDMYJ\Li et al. - 2017 - Learning a model of facial shape and expression fr.pdf}
}

@article{lombardiDeepAppearanceModels2018,
  title = {Deep Appearance Models for Face Rendering},
  author = {Lombardi, Stephen and Saragih, Jason and Simon, Tomas and Sheikh, Yaser},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {4},
  pages = {68:1--68:13},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201401},
  url = {https://dl.acm.org/doi/10.1145/3197517.3201401},
  urldate = {2023-06-13},
  abstract = {We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).},
  keywords = {appearance models,deep appearance models,face rendering,image-based rendering},
  file = {C:\Users\daohi\Zotero\storage\VGVG9MRL\Lombardi et al. - 2018 - Deep appearance models for face rendering.pdf}
}

@report{loThreeDimensionalHairStructure2023,
  type = {Working Paper},
  title = {Three-{{Dimensional Hair Structure Reconstruction}} from a {{Single Sketch Image}} without {{Intermediate Representation}}},
  author = {Lo, Anh Duc and Ma, Thi Chau},
  date = {2023-06},
  institution = {{Vietnam-Korea University of Information and Communication Technology}},
  url = {http://elib.vku.udn.vn/handle/123456789/2700},
  urldate = {2023-11-23},
  abstract = {Reconstructing 3D hair structures from a single image is a highly promissing research direction. However, current methods only focus on real-world images, while user-oriented applications require more freedom in input data. Existing methods for buildings 3D hair from sketchy images use synthetic data for training, which encounters the domain gap issue. We experiment with building a dataset directly from hand-drawn sketches and propose a model trained on it. As a result, without using an intermediate oriented map representation, the model is still able to learn how to reconstruct hair at a satisfactory level. This opens up a new direction for this problem.},
  isbn = {9786048080839},
  langid = {english},
  annotation = {Accepted: 2023-09-25T08:07:21Z},
  file = {C:\Users\daohi\Zotero\storage\5A8XV4YC\Lo and Ma - 2023 - Three-Dimensional Hair Structure Reconstruction fr.pdf}
}

@online{papadopoulosFaceGCNGraphConvolutional2021,
  title = {Face-{{GCN}}: {{A Graph Convolutional Network}} for {{3D Dynamic Face Identification}}/{{Recognition}}},
  shorttitle = {Face-{{GCN}}},
  author = {Papadopoulos, Konstantinos and Kacem, Anis and Shabayek, Abdelrahman and Aouada, Djamila},
  date = {2021-04-20},
  eprint = {2104.09145},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.09145},
  url = {http://arxiv.org/abs/2104.09145},
  urldate = {2023-11-23},
  abstract = {Face identification/recognition has significantly advanced over the past years. However, most of the proposed approaches rely on static RGB frames and on neutral facial expressions. This has two disadvantages. First, important facial shape cues are ignored. Second, facial deformations due to expressions can have an impact on the performance of such a method. In this paper, we propose a novel framework for dynamic 3D face identification/recognition based on facial keypoints. Each dynamic sequence of facial expressions is represented as a spatio-temporal graph, which is constructed using 3D facial landmarks. Each graph node contains local shape and texture features that are extracted from its neighborhood. For the classification/identification of faces, a Spatio-temporal Graph Convolutional Network (ST-GCN) is used. Finally, we evaluate our approach on a challenging dynamic 3D facial expression dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\KEW553VG\\Papadopoulos et al. - 2021 - Face-GCN A Graph Convolutional Network for 3D Dyn.pdf;C\:\\Users\\daohi\\Zotero\\storage\\AFPWSPU5\\2104.html}
}

@inproceedings{pavlakosExpressiveBodyCapture2019,
  title = {Expressive {{Body Capture}}: {{3D Hands}}, {{Face}}, and {{Body From}} a {{Single Image}}},
  shorttitle = {Expressive {{Body Capture}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. and Tzionas, Dimitrios and Black, Michael J.},
  date = {2019-06},
  pages = {10967--10977},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.01123},
  url = {https://ieeexplore.ieee.org/document/8953319/},
  urldate = {2023-05-28},
  abstract = {To facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8× over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\9QUQNQAU\Pavlakos et al. - 2019 - Expressive Body Capture 3D Hands, Face, and Body .pdf}
}

@inproceedings{paysan3DFaceModel2009,
  title = {A {{3D Face Model}} for {{Pose}} and {{Illumination Invariant Face Recognition}}},
  booktitle = {2009 {{Sixth IEEE International Conference}} on {{Advanced Video}} and {{Signal Based Surveillance}}},
  author = {Paysan, Pascal and Knothe, Reinhard and Amberg, Brian and Romdhani, Sami and Vetter, Thomas},
  date = {2009-09},
  pages = {296--301},
  doi = {10.1109/AVSS.2009.58},
  abstract = {Generative 3D face models are a powerful tool in computer vision. They provide pose and illumination invariance by modeling the space of 3D faces and the imaging process. The power of these models comes at the cost of an expensive and tedious construction process, which has led the community to focus on more easily constructed but less powerful models. With this paper we publish a generative 3D shape and texture model, the Basel face model (BFM), and demonstrate its application to several face recognition task. We improve on previous models by offering higher shape and texture accuracy due to a better scanning device and less correspondence artifacts due to an improved registration algorithm. The same 3D face model can be fit to 2D or 3D images acquired under different situations and with different sensors using an analysis by synthesis method. The resulting model parameters separate pose, lighting, imaging and identity parameters, which facilitates invariant face recognition across sensors and data sets by comparing only the identity parameters. We hope that the availability of this registered face model will spur research in generative models. Together with the model we publish a set of detailed recognition and reconstruction results on standard databases to allow complete algorithm comparisons.},
  eventtitle = {2009 {{Sixth IEEE International Conference}} on {{Advanced Video}} and {{Signal Based Surveillance}}},
  keywords = {2D/3D fitting,Basel Face Model (BFM),Basel face model old,Computer vision,Costs,database,Face detection,Face recognition,generative 3D face models,identification,Image analysis,Image reconstruction,Image sensors,Lighting,Morphable Model,Power generation,recognition,Shape,statistical models},
  file = {C:\Users\daohi\Zotero\storage\VVI8D9ZI\5279762.html}
}

@article{ploumpisComplete3DMorphable2021,
  title = {Towards a {{Complete 3D Morphable Model}} of the {{Human Head}}},
  author = {Ploumpis, Stylianos and Ververas, Evangelos and Sullivan, Eimear O' and Moschoglou, Stylianos and Wang, Haoyang and Pears, Nick and Smith, William A. P. and Gecer, Baris and Zafeiriou, Stefanos},
  date = {2021-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {11},
  pages = {4142--4160},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.2991150},
  abstract = {Three-dimensional morphable models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: (i). use a regressor to complete missing parts of one model using the other, and (ii). use the Gaussian Process framework to blend covariance matrices from multiple models. Thus, we build a new combined face-and-head shape model that blends the variability and facial detail of an existing face model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves state-of-the-art performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {3D reconstruction,3DMM,Computational modeling,craniofacial 3DMM,Ear,Face,Magnetic heads,morphable model combination,Shape,Three-dimensional displays},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\JAYHDMQR\\Ploumpis et al. - 2021 - Towards a Complete 3D Morphable Model of the Human.pdf;C\:\\Users\\daohi\\Zotero\\storage\\QMK2C6IV\\9082178.html}
}

@inproceedings{ranjanGenerating3DFaces2018,
  title = {Generating {{3D Faces}} Using {{Convolutional Mesh Autoencoders}}},
  author = {Ranjan, Anurag and Bolkart, Timo and Sanyal, Soubhik and Black, Michael J.},
  date = {2018},
  pages = {704--720},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Anurag_Ranjan_Generating_3D_Faces_ECCV_2018_paper.html},
  urldate = {2023-06-13},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {C:\Users\daohi\Zotero\storage\GNP8PHBC\Ranjan et al. - 2018 - Generating 3D Faces using Convolutional Mesh Autoe.pdf}
}

@inproceedings{RingNet:CVPR:2019,
  title = {Learning to Regress {{3D}} Face Shape and Expression from an Image without {{3D}} Supervision},
  booktitle = {Proceedings {{IEEE}} Conf. on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Sanyal, Soubhik and Bolkart, Timo and Feng, Haiwen and Black, Michael},
  date = {2019-06},
  pages = {7763--7772},
  month_numeric = {6}
}

@online{saitoPIFuPixelAlignedImplicit2019,
  title = {{{PIFu}}: {{Pixel-Aligned Implicit Function}} for {{High-Resolution Clothed Human Digitization}}},
  shorttitle = {{{PIFu}}},
  author = {Saito, Shunsuke and Huang, Zeng and Natsume, Ryota and Morishima, Shigeo and Kanazawa, Angjoo and Li, Hao},
  date = {2019-12-03},
  eprint = {1905.05172},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.05172},
  urldate = {2023-11-23},
  abstract = {We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\PGV8NJE3\\Saito et al. - 2019 - PIFu Pixel-Aligned Implicit Function for High-Res.pdf;C\:\\Users\\daohi\\Zotero\\storage\\LSJRBQWH\\1905.html}
}

@online{sanyalLearningRegress3D2019,
  title = {Learning to {{Regress 3D Face Shape}} and {{Expression}} from an {{Image}} without {{3D Supervision}}},
  author = {Sanyal, Soubhik and Bolkart, Timo and Feng, Haiwen and Black, Michael J.},
  date = {2019-05-16},
  url = {https://arxiv.org/abs/1905.06817v1},
  urldate = {2023-11-22},
  abstract = {The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual's face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces `not quite in-the-wild' (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes at http://ringnet.is.tuebingen.mpg.de.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C:\Users\daohi\Zotero\storage\CFA2Z33D\Sanyal et al. - 2019 - Learning to Regress 3D Face Shape and Expression f.pdf}
}

@inproceedings{smithMorphableFaceAlbedo2020,
  title = {A {{Morphable Face Albedo Model}}},
  author = {Smith, William A. P. and Seck, Alassane and Dee, Hannah and Tiddeman, Bernard and Tenenbaum, Joshua B. and Egger, Bernhard},
  date = {2020},
  pages = {5011--5020},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Smith_A_Morphable_Face_Albedo_Model_CVPR_2020_paper.html},
  urldate = {2023-06-14},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\daohi\Zotero\storage\KQ4HBYGF\Smith et al. - 2020 - A Morphable Face Albedo Model.pdf}
}

@article{thiesRealtimeExpressionTransfer2015,
  title = {Real-Time Expression Transfer for Facial Reenactment},
  author = {Thies, Justus and Zollhöfer, Michael and Nießner, Matthias and Valgaerts, Levi and Stamminger, Marc and Theobalt, Christian},
  date = {2015-11-02},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {34},
  number = {6},
  pages = {183:1--183:14},
  issn = {0730-0301},
  doi = {10.1145/2816795.2818056},
  url = {https://doi.org/10.1145/2816795.2818056},
  urldate = {2023-06-13},
  abstract = {We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor. The novelty of our approach lies in the transfer and photorealistic re-rendering of facial deformations and detail into the target video in a way that the newly-synthesized expressions are virtually indistinguishable from a real video. To achieve this, we accurately capture the facial performances of the source and target subjects in real-time using a commodity RGB-D sensor. For each frame, we jointly fit a parametric model for identity, expression, and skin reflectance to the input color and depth data, and also reconstruct the scene lighting. For expression transfer, we compute the difference between the source and target expressions in parameter space, and modify the target parameters to match the source expressions. A major challenge is the convincing re-rendering of the synthesized target face into the corresponding video stream. This requires a careful consideration of the lighting and shading design, which both must correspond to the real-world environment. We demonstrate our method in a live setup, where we modify a video conference feed such that the facial expressions of a different person (e.g., translator) are matched in real-time.},
  keywords = {depth camera,expression transfer,faces,real-time}
}

@article{tretschkStateArtDense2023,
  title = {State of the {{Art}} in {{Dense Monocular Non-Rigid 3D Reconstruction}}},
  author = {Tretschk, Edith and Kairanda, Navami and B R, Mallikarjun and Dabral, Rishabh and Kortylewski, Adam and Egger, Bernhard and Habermann, Marc and Fua, Pascal and Theobalt, Christian and Golyanik, Vladislav},
  date = {2023},
  journaltitle = {Computer Graphics Forum},
  volume = {42},
  number = {2},
  pages = {485--520},
  issn = {1467-8659},
  doi = {10.1111/cgf.14774},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14774},
  urldate = {2023-05-28},
  abstract = {3D reconstruction of deformable (or non-rigid) scenes from a set of monocular 2D image observations is a long-standing and actively researched area of computer vision and graphics. It is an ill-posed inverse problem, since—without additional prior assumptions—it permits infinitely many solutions leading to accurate projection to the input 2D images. Non-rigid reconstruction is a foundational building block for downstream applications like robotics, AR/VR, or visual content creation. The key advantage of using monocular cameras is their omnipresence and availability to the end users as well as their ease of use compared to more sophisticated camera set-ups such as stereo or multi-view systems. This survey focuses on state-of-the-art methods for dense non-rigid 3D reconstruction of various deformable objects and composite scenes from monocular videos or sets of monocular views. It reviews the fundamentals of 3D reconstruction and deformation modeling from 2D image observations. We then start from general methods—that handle arbitrary scenes and make only a few prior assumptions—and proceed towards techniques making stronger assumptions about the observed objects and types of deformations (e.g. human faces, bodies, hands, and animals). A significant part of this STAR is also devoted to classification and a high-level comparison of the methods, as well as an overview of the datasets for training and evaluation of the discussed techniques. We conclude by discussing open challenges in the field and the social aspects associated with the usage of the reviewed methods.},
  langid = {english},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\HNLKDAMT\\Tretschk et al. - 2023 - State of the Art in Dense Monocular Non-Rigid 3D R.pdf;C\:\\Users\\daohi\\Zotero\\storage\\ZNDT5FSP\\cgf.html}
}

@article{wardSurveyHairModeling2007,
  title = {A {{Survey}} on {{Hair Modeling}}: {{Styling}}, {{Simulation}}, and {{Rendering}}},
  shorttitle = {A {{Survey}} on {{Hair Modeling}}},
  author = {Ward, Kelly and Bertails, Florence and Kim, Tae-yong and Marschner, Stephen R. and Cani, Marie-paule and Lin, Ming C.},
  date = {2007-03},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {13},
  number = {2},
  pages = {213--234},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2007.30},
  url = {https://ieeexplore.ieee.org/document/4069232},
  urldate = {2023-11-23},
  abstract = {Realistic hair modeling is a fundamental part of creating virtual humans in computer graphics. This paper surveys the state of the art in the major topics of hair modeling: hairstyling, hair simulation, and hair rendering. Because of the difficult, often unsolved problems that arise in alt these areas, a broad diversity of approaches is used, each with strengths that make it appropriate for particular applications. We discuss each of these major topics in turn, presenting the unique challenges facing each area and describing solutions that have been presented over the years to handle these complex issues. Finally, we outline some of the remaining computational challenges in hair modeling},
  eventtitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  file = {C:\Users\daohi\Zotero\storage\TJB2NJCT\Ward et al. - 2007 - A Survey on Hair Modeling Styling, Simulation, an.pdf}
}

@inproceedings{xuDeep3DPortrait2020,
  title = {Deep {{3D Portrait From}} a {{Single Image}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Sicheng and Yang, Jiaolong and Chen, Dong and Wen, Fang and Deng, Yu and Jia, Yunde and Tong, Xin},
  date = {2020-06},
  pages = {7707--7717},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00773},
  url = {https://ieeexplore.ieee.org/document/9156419/},
  urldate = {2023-06-10},
  abstract = {In this paper, we present a learning-based approach for recovering the 3D geometry of human head from a single portrait image. Our method is learned in an unsupervised manner without any ground-truth 3D data. We represent the head geometry with a parametric 3D face model together with a depth map for other head regions including hair and ear. A two-step geometry learning scheme is proposed to learn 3D head reconstruction from in-the-wild face images, where we first learn face shape on single images using selfreconstruction and then learn hair and ear geometry using pairs of images in a stereo-matching fashion. The second step is based on the output of the first to not only improve the accuracy but also ensure the consistency of overall head geometry. We evaluate the accuracy of our method both in 3D and with pose manipulation tasks on 2D images. We alter pose based on the recovered geometry and apply a refinement network trained with adversarial learning to ameliorate the reprojected images and translate them to the real image domain. Extensive evaluations and comparison with previous methods show that our new method can produce high-fidelity 3D head geometry and head pose manipulation results.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\82DN8INS\Xu et al. - 2020 - Deep 3D Portrait From a Single Image.pdf}
}

@inproceedings{yangFaceScapeLargeScaleHigh2020,
  title = {{{FaceScape}}: {{A Large-Scale High Quality 3D Face Dataset}} and {{Detailed Riggable 3D Face Prediction}}},
  shorttitle = {{{FaceScape}}},
  author = {Yang, Haotian and Zhu, Hao and Wang, Yanru and Huang, Mingkai and Shen, Qiu and Yang, Ruigang and Cao, Xun},
  date = {2020},
  pages = {601--610},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_FaceScape_A_Large-Scale_High_Quality_3D_Face_Dataset_and_Detailed_CVPR_2020_paper.html},
  urldate = {2023-06-13},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C:\Users\daohi\Zotero\storage\FB7E3X7F\Yang et al. - 2020 - FaceScape A Large-Scale High Quality 3D Face Data.pdf}
}

@inproceedings{zeng3DHumanMesh2020,
  title = {{{3D Human Mesh Regression With Dense Correspondence}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zeng, Wang and Ouyang, Wanli and Luo, Ping and Liu, Wentao and Wang, Xiaogang},
  date = {2020-06},
  pages = {7052--7061},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00708},
  url = {https://ieeexplore.ieee.org/document/9157190/},
  urldate = {2023-11-23},
  abstract = {Estimating 3D mesh of the human body from a single 2D image is an important task with many applications such as augmented reality and Human-Robot interaction. However, prior works reconstructed 3D mesh from global image feature extracted by using convolutional neural network (CNN), where the dense correspondences between the mesh surface and the image pixels are missing, leading to suboptimal solution. This paper proposes a model-free 3D human mesh estimation framework, named DecoMR, which explicitly establishes the dense correspondence between the mesh and the local image features in the UV space (i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts pixel-to-surface dense correspondence map (i.e., IUV image), with which we transfer local features from the image space to the UV space. Then the transferred local image features are processed in the UV space to regress a location map, which is well aligned with transferred features. Finally we reconstruct 3D human mesh from the regressed location map with a predefined mapping function. We also observe that the existing discontinuous UV map are unfriendly to the learning of network. Therefore, we propose a novel UV map that maintains most of the neighboring relations on the original mesh surface. Experiments demonstrate that our proposed local feature alignment and continuous UV map outperforms existing 3D mesh based methods on multiple public benchmarks. Code will be made available at https: //github.com/zengwang430521/DecoMR.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\IP4CZA9P\Zeng et al. - 2020 - 3D Human Mesh Regression With Dense Correspondence.pdf}
}

@online{zhangUnreasonableEffectivenessDeep2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018-04-10},
  eprint = {1801.03924},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.03924},
  url = {http://arxiv.org/abs/1801.03924},
  urldate = {2023-11-22},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\AYEQ8L9K\\Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf;C\:\\Users\\daohi\\Zotero\\storage\\W3M34W8U\\1801.html}
}

@inproceedings{zhouHairNetSingleViewHair2018,
  title = {{{HairNet}}: {{Single-View Hair Reconstruction Using Convolutional Neural Networks}}},
  shorttitle = {{{HairNet}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Zhou, Yi and Hu, Liwen and Xing, Jun and Chen, Weikai and Kung, Han-Wei and Tong, Xin and Li, Hao},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {249--265},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01252-6_15},
  abstract = {We introduce a deep learning-based method to generate full 3D hair geometry from an unconstrained image. Our method can recover local strand details and has real-time performance. State-of-the-art hair modeling techniques rely on large hairstyle collections for nearest neighbor retrieval and then perform ad-hoc refinement. Our deep learning approach, in contrast, is highly efficient in storage and can run 1000 times faster while generating hair with 30K strands. The convolutional neural network takes the 2D orientation field of a hair image as input and generates strand features that are evenly distributed on the parameterized 2D scalp. We introduce a collision loss to synthesize more plausible hairstyles, and the visibility of each strand is also used as a weight term to improve the reconstruction accuracy. The encoder-decoder architecture of our network naturally provides a compact and continuous representation for hairstyles, which allows us to interpolate naturally between hairstyles. We use a large set of rendered synthetic hair models to train our network. Our method scales to real images because an intermediate 2D orientation field, automatically calculated from the real image, factors out the difference between synthetic and real hairs. We demonstrate the effectiveness and robustness of our method on a wide range of challenging real Internet pictures, and show reconstructed hair sequences from videos.},
  isbn = {978-3-030-01252-6},
  langid = {english},
  keywords = {DNN,Hair,Real-time,Reconstruction},
  file = {C:\Users\daohi\Zotero\storage\K55IKJYF\Zhou et al. - 2018 - HairNet Single-View Hair Reconstruction Using Con.pdf}
}

@online{zielonkaMetricalReconstructionHuman2022,
  title = {Towards {{Metrical Reconstruction}} of {{Human Faces}}},
  author = {Zielonka, Wojciech and Bolkart, Timo and Thies, Justus},
  date = {2022-10-19},
  eprint = {2204.06607},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.06607},
  urldate = {2023-06-01},
  abstract = {Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15\% and 24\% lower average error on NoW, respectively).},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,MICA},
  file = {C\:\\Users\\daohi\\Zotero\\storage\\WLU69P6C\\Zielonka et al. - 2022 - Towards Metrical Reconstruction of Human Faces.pdf;C\:\\Users\\daohi\\Zotero\\storage\\SRY8MIDS\\2204.html}
}

@article{zollhoferStateArtMonocular2018,
  title = {State of the {{Art}} on {{Monocular 3D Face Reconstruction}}, {{Tracking}}, and {{Applications}}},
  author = {Zollhöfer, M. and Thies, J. and Garrido, P. and Bradley, D. and Beeler, T. and Pérez, P. and Stamminger, M. and Nießner, M. and Theobalt, C.},
  date = {2018-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {37},
  number = {2},
  pages = {523--550},
  issn = {01677055},
  doi = {10.1111/cgf.13382},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13382},
  urldate = {2023-05-30},
  abstract = {The computer graphics and vision communities have dedicated long standing efforts in building computerized tools for reconstructing, tracking, and analyzing human faces based on visual input. Over the past years rapid progress has been made, which led to novel and powerful algorithms that obtain impressive results even in the very challenging case of reconstruction from a single RGB or RGB-D camera. The range of applications is vast and steadily growing as these technologies are further improving in speed, accuracy, and ease of use.},
  langid = {english},
  file = {C:\Users\daohi\Zotero\storage\LHWWME8C\Zollhöfer et al. - 2018 - State of the Art on Monocular 3D Face Reconstructi.pdf}
}
