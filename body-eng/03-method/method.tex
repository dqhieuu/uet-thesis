\section{The method}
\label{sec:method}

\subsection{Requirements analysis}
\subsubsection{Introduction}
This thesis aims to create a system that can reconstruct a 3D avatar from a single-view portrait image. The system should be able to handle a variety of tasks related to 3D avatar creation, including:
\begin{itemize}
    \item Creating a 3D avatar from a single-view portrait image

    \item Customizing the 3D avatar's facial expression

    \item Trying on different hairstyles on the 3D avatar
\end{itemize}

From the requirements above, an analysis of the system's requirements is conducted to determine the system's architecture and the methods used to create the system. This section clarifies the requirements analysis and the system's architecture, using UML diagrams.

\subsubsection{Use cases}
The use cases of the system are shown in the figure below:
\clearpage

\begin{umlfigure}[Use cases of the system][use_cases]
    @startuml
    left to right direction
    actor User
    rectangle "3D Avatar Reconstruction System" {
    usecase "Create 3D avatar from a portrait image" as UC1
    usecase "Create 3D avatar with customized emotions" as UC2
    usecase "Try on different hairstyles with 3D avatar" as UC3
    }
    User --> UC1
    User --> UC2
    User --> UC3
    @enduml
\end{umlfigure}

\subsubsubsection{Create 3D avatar from a portrait image}
This use case is the main use case of the system. The system should provide a GUI (graphical user interface) to allow the user to upload their images easily. The user uploads a single-view portrait image to the system, and the system will process the image and output a 3D avatar in the form of a 3D mesh reconstructed from the input image. The user can choose to download the 3D avatar as a zip file, which contains the 3D avatar model and texture map. The sequence diagram of this use case is shown in the figure below.

\begin{umlfigure}[Analysis: Sequence diagram: Create 3D avatar from a portrait image.][seq_create_3d_avatar]
    @startuml
    actor User
    participant "3D Avatar Reconstruction System" as System
    User -> System: Uploads image
    System -> System: Processes image
    System -> User: Outputs 3D avatar
    User -> User: Views 3D avatar
    User -> System: Downloads 3D avatar
    @enduml
\end{umlfigure}


\subsubsubsection{Create 3D avatar with customized emotions}

This use case is an extension of the main use case. The user can choose to customize the 3D avatar's emotion by using sliders to adjust the intensity of a set of emotions. The system will then output a 3D avatar with the emotions applied. The user can choose to download the 3D avatar as a zip file, which contains the 3D avatar model and texture map. The sequence diagram of this use case is shown in the figure below.

\begin{umlfigure}[Analysis: Sequence diagram: Create 3D avatar with customized emotions.][seq_create_3d_avatar_emotion]
    @startuml
    actor User
    participant "3D Avatar Reconstruction System" as System
    User -> System: Uploads image
    System -> System: Processes image
    System -> User: Outputs 3D avatar
    User -> System: Adjust emotion
    System -> System: Apply emotion parameters to 3D avatar
    System -> User: Outputs emotion-applied 3D avatar
    User -> User: View 3D avatar
    User -> System: Download 3D avatar
    @enduml

\end{umlfigure}

\subsubsubsection{Try on different hairstyles with 3D avatar}

This use case is an extension of the main use case. The user can choose to try on different hairstyles with the 3D avatar. The system will then output a 3D avatar with the selected hairstyle. The user can choose to download the 3D avatar as a zip file, which contains the 3D avatar model, the texture map, and the hairstyle model. The sequence diagram of this use case is shown in the figure below.

\begin{umlfigure}[Analysis: Sequence diagram: Try on different hairstyles with 3D avatar.][seq_try_on_hairstyle]
    @startuml
    actor User
    participant "3D Avatar Reconstruction System" as System
    User -> System: Uploads image
    System -> System: Processes image
    System -> User: Outputs 3D avatar
    User -> System: Select hairstyle
    System -> System: Apply hairstyle to 3D avatar
    System -> User: Outputs 3D avatar with hairstyle
    User -> User: View 3D avatar
    User -> System: Download 3D avatar
    @enduml
\end{umlfigure}

\subsection{System architecture overview}

\subsubsection{Overall flow}

Essentially, the system architecture serves the purpose of taking a single-view portrait image of a person and outputs a 3D avatar reconstructed from the input image. The overview of the system flow and the decision tree corresponding to the user's options are shown in the figure below.

\includefigure[The system flow overview, with options for reconstruction output.]{images/flow_newest_21.10.png}

The system holds a database for hairstyles, which is convenient for try-on purposes. Instead of going through the standard flow where the system extracts the user's hairstyle from the captured image, the user can try on a variety of hairstyles in the database to see if any of these hairstyles suit their face.

The details of each reconstruction block will be explained in detail in the sections below, where the red blocks are the blocks that are novel and implemented in this thesis.

\subsubsection{Flow for use case: Create 3D avatar from a portrait image}
For taking a single-view portrait image as input, the system provides a front-end interface, i.e. a web page, where the user can upload their image. The front-end interface allows the user to send the image to a gateway server, which is responsible for handling the user's requests and sending the image to the back-end server for processing. By using a gateway server, the system architecture can be modularized, which means multiple reconstruction backends can be used as substitutions. A gateway also allows the system to easily scale up to handle a large number of requests by adding more back-end servers.

The back-end server is responsible for reconstruction tasks and algorithmic tasks. To be able to create a 3D avatar from a single-view portrait image, the system needs to be able to reconstruct the 3D head model from the input image. The system uses a pre-trained DECA model and can be substituted by a pre-trained MICA model \cite{fengLearningAnimatableDetailed2021} to reconstruct the 3D head model.

The implemented flow for creating a 3D avatar from a single-view portrait image is shown in the figure below.

\begin{umlfigure}[Implementation: Sequence diagram: Create 3D avatar from a portrait image.][seq_create_3d_avatar_implementation]
    @startuml
    actor User
    boundary "Front-end interface" as Frontend
    control "Gateway server" as Gateway
    control "DECA server" as DECA
    control "MICA server" as MICA


    alt use DECA to reconstruct face
    User -> Frontend: Uploads image
    Frontend -> Gateway: Sends image
    Gateway -> DECA: Sends image
    DECA -> DECA: Processes image
    DECA -> Gateway: Sends 3D avatar
    else use MICA to reconstruct face
    User -> Frontend: Uploads image
    Frontend -> Gateway: Sends image
    Gateway -> MICA: Sends image
    MICA -> MICA: Processes image
    MICA -> Gateway: Sends 3D avatar
    end

    Gateway -> Frontend: Sends 3D avatar
    Frontend -> User: Outputs 3D avatar
    User -> User: Views 3D avatar

    opt User wants to download 3D avatar
    User -> Frontend: Downloads 3D avatar
    end
    @enduml
\end{umlfigure}

\clearpage

\subsubsection{Flow for use case: Create 3D avatar with customized emotions}
The system allows the user to customize the 3D avatar's emotion by using sliders to adjust the intensity of a set of emotions. The system uses a simple emotion-to-FLAME regressive model to convert the emotion parameters to FLAME's pose and expression parameters. The emotion-to-FLAME regressive model is trained on the VKIST dataset, with the ground truth acquired by running the dataset through the pre-trained face reconstruction model (DECA, MICA, or EMOCA). The implemented flow for creating a 3D avatar with customized emotions is shown in the figure below.

\clearpage

\begin{umlfigure}*[Implementation: Sequence diagram: Create 3D avatar with customized emotions.][seq_create_3d_avatar_emotion_implementation]
    @startuml
    actor User
    boundary "Front-end interface" as Frontend
    control "Gateway server" as Gateway
    control "Face reconstruction server" as Face
    control "Emotion-to-FLAME server" as Emotion2FLAME

    User -> Frontend: Uploads image
    Frontend -> Gateway: Sends image
    Gateway -> Face: Sends image
    Face -> Face: Processes image
    Face -> Gateway: Sends 3D avatar
    Gateway -> Frontend: Sends 3D avatar
    Frontend -> User: Outputs 3D avatar
    User -> Frontend: Adjust emotion
    Frontend -> Gateway: Sends emotion parameters
    Gateway -> Emotion2FLAME: Sends emotion parameters
    Emotion2FLAME -> Emotion2FLAME: Infers FLAME parameters
    Emotion2FLAME -> Face: Sends FLAME parameters
    Face -> Face: Applies FLAME parameters to 3D avatar
    Face -> Gateway: Sends emotion-applied 3D avatar
    Gateway -> Frontend: Sends 3D avatar
    Frontend -> User: Outputs emotion-applied 3D avatar
    User -> User: View 3D avatar

    opt User wants to download 3D avatar
    User -> Frontend: Download 3D avatar
    end
    @enduml
\end{umlfigure}

\clearpage

\subsubsection{Flow for use case: Try on different hairstyles with 3D avatar}

The system allows the user to try on different hairstyles with the 3D avatar. The system uses a pre-trained image-to-hair model to reconstruct the user's hairstyle from the input image. If the user chooses to try on different hairstyles, the system will not use the user's hairstyle from the input image. Instead, the system will use one of the hairstyles in the database. The user can choose to download the 3D avatar with the hairstyle as a zip file, which contains the 3D avatar model, the texture map, and the hairstyle model. The implemented flow for trying on different hairstyles with the 3D avatar is shown in the figure below.

\clearpage

\begin{umlfigure}*[Implementation: Sequence diagram: Try on different hairstyles with 3D avatar.][seq_try_on_hairstyle_implementation]
    @startuml
    actor User
    boundary "Front-end interface" as Frontend
    control "Gateway server" as Gateway
    control "Face reconstruction server" as Face
    control "Image-to-hair server" as Hair
    database "Hairstyle database" as DB

    User -> Frontend: Uploads image
    Frontend -> Gateway: Sends image
    Gateway -> Face: Sends image
    Face -> Face: Processes image
    Face -> Gateway: Sends 3D avatar
    Gateway -> Frontend: Sends 3D avatar
    Frontend -> User: Outputs 3D avatar


    opt User wants to reconstruct hairstyle from image
    Gateway -> Hair: Sends image
    Hair -> Hair: Processes hairstyle
    Hair -> Gateway: Sends hairstyle
    else User wants to try on different hairstyles
    User -> Frontend: Select hairstyle
    Frontend -> Gateway: Sends hairstyle ID
    Gateway -> DB: Select hairstyle
    end



    Gateway -> Frontend: Sends hairstyle
    Frontend -> User: Outputs 3D avatar with hairstyle
    User -> User: View 3D avatar

    opt User wants to download 3D avatar
    User -> Frontend: Download 3D avatar
    end
    @enduml
\end{umlfigure}

\clearpage

\subsection{System architecture and algorithm details}

\includefigure[The pipeline of the proposed system.]{images/system_pipeline.png}


\begin{itemize}
    \item The system takes an image input with an API endpoint
    \item The image-to-head encoder outputs the FLAME's shape, pose, and expression parameters, and the extracted face texture.
    \item Optionally, the system can take human-friendly emotion parameters to output FLAME's pose and expression parameters, using a simple emotion-to-FLAME regressive model.
    \item The FLAME-to-output decoder takes FLAME's shape, pose, and expression parameters, texture coordinate, and extracted image texture to output a 3D model with a texture map and a normal map.
    \item While the head is being processed, the image-to-hair model outputs the reconstructed strand-based 3D hair model.
    \item Optionally, a 3D hair model can be chosen from the database instead of using the image-to-hair model for the try-on purpose.
    \item After the head model and the hair model are generated, they are combined with an alignment procedure to create an accurate 3D avatar zip file.
    \item Finally, the zip file is sent to the user, where the 3D renderer on the user's web browser will be used to render the 3D avatar.
\end{itemize}

\subsubsection{Face reconstruction}
- Outputs FLAME as the base model
- Uses DECA to reconstruct the face, can combine with MICA for better shape reconstruction
- DECA outputs texture and shape, and normal map, and displacement map


\subsubsection{Hair reconstruction and using hairstyles from the database}
- Uses Hairnet to output strand-based hair model
- Can choose to use hairstyles from the database instead of using Hairnet

\subsubsection{Face and hair alignment}
To make our system a fully automated avatar reconstruction system, the hair model and head model need to be automatically aligned with each other. To achieve this, we propose an automated procedure for aligning the hair model with the head model.
First, we define the anchor point of the hair object and the head object (Fig \ref{head_hair_sampled}) as the highest point of the mesh and the center of the mesh, respectively, from the top view.

For the head mesh $M_{head}=\{p_1,\ p_2,...,\ p_N\}$, we sample 5\% of the points with the highest z-axis values to create the set $M_{head\_5\%}$. For the hair mesh $M_{hair}$, for every hair strand, we sample 5\% of the points that start from the strand root to create the set $M_{hair\_5\%}$. We find that 5\% is enough to sample the points that form the head top, which can be used to approximate the center of the head and the hair accurately.

\includefigure[Sampled points in hair and head models to calculate the anchor points.]{images/head_hair_sampled.png}

We then calculate the hair anchor point $A_{hair}$ and the head anchor point $A_{head}$ by taking the means of the x-axis and the y-axis and the highest value of the z-axis.

\begin{equation}
    A = \begin{pmatrix}
        x_{mean\_5\%} \\
        y_{mean\_5\%} \\
        z_{max\_5\%}
    \end{pmatrix}
    = \begin{pmatrix}
        \frac{x_{p1}+x_{p2}+...+x_{pN5\%}}{0.05N} \\
        \frac{y_{p1}+y_{p2}+...+y_{pN5\%}}{0.05N} \\
        \max(z_{p1},z_{p2},...,y_{pN_{5\%}})
    \end{pmatrix}
\end{equation}

With the hair anchor and the head anchor, the translation vector $T_{hair}$ (Fig \ref{hair_translation}) is calculated by taking the subtraction of the 2 anchor points and adding it by a constant calibration value $C$.

\begin{equation}
    T_{hair} = A_{hair} - A_{head} + C
\end{equation}

For the scaling adjustment, first, I find the axis-aligned bounding boxes of the hair model and the head model. An assumption is made, which is the head model and the hair model are front-facing, as per the requirement of the captured image. Thus, the front-view lengths of the hair and the head can be deduced by taking the x-axis lengths. With the hair and the head length, the scaling coefficient can be calculated by taking the ratio of the hair length to the head length and can be adjusted with the addition constant $C$.

\begin{equation}
    S_{hair} = \frac{W_{hair}}{W_{head}} + C
\end{equation}

The final adjustment formula can be calculated with the translation vector $T_{hair}$ and the scaling coefficient $S_{hair}$

\begin{equation}
    p_{new} = \frac{1}{S_{hair}}\cdot(p_{old} + T_{hair})
\end{equation}

By aligning the hair model with the already aligned head model, the problem of Hairnet being unable to align with the captured image as mentioned in Xu et al.'s paper \cite{xuDeep3DPortrait2020} is addressed.


\includefigure[The alignment of the hair model with the head model by translation.]{images/hair_translation.png}

\subsection{Contribution: Customizable facial emotion}


\subsubsection{The idea}
% - With VKIST dataset, get a total of 6 emotion parameters: happy, sad, angry, surprised, disgusted, fearful
% - Train a simple emotion-to-FLAME regressive model to convert emotion parameters to FLAME's pose and expression parameters
% - The emotion-to-FLAME regressive model is trained on the VKIST dataset, with the ground truth acquired by running the dataset through the pre-trained face reconstruction model (DECA, MICA, or EMOCA).
% - Two models are trained: one with emotion parameters as input and FLAME's pose and expression parameters as output, and one with emotion parameters plus FLAME's identity (shape) parameters as input and FLAME's pose and expression parameters as output.

The idea is to create a simple emotion-to-FLAME regressive model to convert emotion parameters to FLAME's pose and expression parameters. The emotion-to-FLAME regressive model is trained on the VKIST dataset, with the ground truth acquired by running the dataset through the pre-trained face reconstruction model (DECA, MICA, or EMOCA). Two models are trained: one with emotion parameters as input and FLAME's pose and expression parameters as output, and one with emotion parameters plus FLAME's identity (shape) parameters as input and FLAME's pose and expression parameters as output.




\subsubsection{Model architecture}

The emotion-to-FLAME regressive model is a simple feed-forward model with 6 input nodes and 56 output nodes, where the first 50 nodes are expression parameters, and the next 6 nodes are pose parameters. The model architecture is shown in the figure below.

\includefigure[The emotion-to-FLAME model architecture.]{images/emotion_net.png}

The "linear" layers perform linear transformations on the input data, which means they apply a weight matrix and a bias vector to the input. The ReLU activation function is used to introduce non-linearity into the network. Since the emotion parameters in FLAME are projected linearly to the mesh vertices, the linear layers perform well as a linear regressor.

The loss function used is the mean absolute error (MAE/L1) loss function $L_{MAE}$, which is defined as:

\begin{equation}
    L_{MAE} = \frac{1}{N}\sum_{i=1}^{N}|y_i-\hat{y_i}|
\end{equation}
where $y_i$ is the ground truth, $\hat{y_i}$ is the predicted value, and $N$ is the number of samples.

With the output of the emotion-to-FLAME model, the loss function is defined as:

\begin{equation}
    \begin{split}
        Loss & = Loss(P_{exp},\hat{P_{exp}}) + Loss(P_{pose},\hat{P_{pose}})                               \\
             & = L_{MAE}(P_{exp},\hat{P_{exp}}) + \alpha\cdot L_{MAE}(P_{neck\_pose},\hat{P_{neck\_pose}}) \\
             & \quad + \beta\cdot L_{MAE}(P_{global\_pose},\hat{P_{global\_pose}})
    \end{split}
\end{equation}
where $P_{exp}$ is the ground truth expression parameters, $\hat{P_{exp}}$ is the predicted expression parameters, $P_{pose}$ is the ground truth pose parameters, $\hat{P_{pose}}$ is the predicted pose parameters. Within pose parameters, there are neck pose parameters and global pose parameters. $\alpha$ and $\beta$ are the weights for the neck pose and global pose loss, respectively.

An alternative architecture which t