\section{Results and discussion}
\label{sec:results}

\subsection{Data Description}
\subsubsection{Hair reconstruction}
\subsubsubsection{Training}
To train the hair reconstruction model, we used the public HairNet dataset \cite{zhouHairNetSingleViewHair2018}. Using our generation method, we were able to create a dataset of roughly 30,000 images for training the hair reconstruction model from the database of 343 hair models.

\subsubsection{Face reconstruction}
\subsubsubsection{Training}
We use the pre-trained DECA model for face reconstruction. The system can swap DECA with other face reconstruction models such as MICA \cite{zielonkaMetricalReconstructionHuman2022} or EMOCA. We then apply the base face reconstruction model with our emotion model to generate the final avatar model.

\subsubsection{Avatar reconstruction}
Avatar reconstruction is achieved with the combination of hair reconstruction and face reconstruction. We use the pre-trained DECA (which can be swapped with MICA, EMOCA) model for face reconstruction, a Hairnet-based architecture for hair reconstruction. We then apply the base face reconstruction model with our emotion model to generate the final avatar model.

\subsubsubsection{Evaluation}
To evaluate the system, we used face images generated from a StyleGAN \cite{karrasStyleBasedGeneratorArchitecture2019} model. These images are diverse in ethnicity, gender, age, and other attributes, making them suitable for evaluating the realism and accuracy of our system.

\subsubsection{Customizable facial emotions}
\subsubsubsection{Training}
To train the emotion network, we use the VKIST dataset which consists of 889 images of 127 subjects with 7 captured emotions (neutral, anger, disgust, fear, happiness, sadness, surprise). All images are captured from the front view with a resolution of $2976\times1984$. The link to download the dataset is available at (\url{https://tinyurl.com/vkist-face-front-02}).

90\% of the images are used for training and 10\% for testing. From the dataset, we extract the face region and resize it to $256\times256$. We then use the pre-trained DECA model to extract the expression parameters from the face images. These expression parameters serve as the ground truth for the emotion model.

\includefigure[Sample images in the VKIST dataset.]{images/vkist_dataset.jpg}[1.0]

\subsubsubsection{Evaluation}
To evaluate the emotion model, we use the FaceScape dataset, which consists of 847 subjects, is used. The dataset consists of scanned 3D face models of 20 different expressions, and 56 images corresponding to 56 camera angles to create each of the scanned models. Only the data of 359 subjects is used because the remaining subjects aren't provided with the captured images/expressions, which makes it more difficult to evaluate the effectiveness of the method.

\subsection{Experimental Scenarios}
To evaluate the effectiveness of our system, we conducted two experiments. The first experiment is to evaluate the quality of the avatar generated by our system. The second experiment is to evaluate the quality of the customizable facial emotions.


\subsection{Evaluation Methods}
\subsubsection{Avatar reconstruction}
To evaluate the quality of the avatar generated by our system, we have designed a qualitative survey using a Likert scale with a 5-point rating system (1-5). The survey question asks respondents to rate the degree of similarity between the original input and the avatar output. This question serves as a key metric for assessing the effectiveness of our proposal.
Using this question, we aimed to assess how closely the avatar output resembles the original input from the perspective of the survey participants.
\subsubsubsection{Measurements: DIFD}
The DIFD (Difference in Facial Descriptors) evaluation method determines whether two portrait images belong to the same person by comparing the difference between their embedding vectors using the Facenet model. Similar to FaceNet, we determine that two portrait images belong to the same person if their DIFD score is less than 1.5.

\subsubsubsection{Measurements: PSNR, SSIM, LPIPS}
PSNR and SSIM are widely used non-deep learning methods that measure similarity based on specific image attributes and provide information about the similarity in terms of noise and structure.
LPIPS is a deep learning-based metric that employs a neural network to learn image features and compute the similarity between two images based on these features.

\subsubsection{Customizable facial emotions}
To evaluate the quality of the customization of the facial emotions, we follow the method used in the NoW challenge and compare the results of different method outputs with the ground truth. The ground truth is the scanned meshes of the FaceScape dataset, compared with the reconstructed face models.

\subsubsubsection{Measurements: The NoW challenge}
The NoW challenge provides a benchmark method specialized in measuring the accuracy and robustness of 3D face reconstruction methods. It uses a set of 7 landmark points to rigidly align the predicted mesh with the ground truth mesh. The landmark points are the leftmost and the rightmost points of the two eyes, the tip of the nose, the leftmost point, and the rightmost point of the mouth. The error is then calculated using the absolute distance between each scan vertex and the closest point in the mesh surface.

\includefigure[The area used for evaluation in the NoW challenge.]{images/now_dataset.png}

\subsection{Experimental Results and Commentary}
\subsubsection{Avatar reconstruction}

\subsubsubsection{Quantitative results}
Table \ref{loss_stats} illustrates the results of the aforementioned measurements when comparing our method with several other 3D face reconstruction methods. The results show that, in terms of comparison, our results are not as good as many other methods such as i3DMM and MoFaNeRF because they applied the measurements to hairless faces. However, we also see that all of the measurement results meet the requirements. In particular, the average value of DIFD is 0.25, which indicates that the synthesized output image has been evaluated as retaining the represented features of the same person as the input image.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{Comparison of our proposal and others}
    \begin{tabularx}{\linewidth}{| X | X | X | X | X |}
        \hline
                            & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} & \textbf{DIFD} \\ \hline\hline
        \textbf{Our system} & 23.15         & 0.835         & 0.09           & 0.25          \\ \hline %0.2521 
        \textbf{i3DMM}      & 24.45         & 0.904         & 0.11           & NA            \\ \hline
        \textbf{MoFaNeRF}   & 31.49         & 0.951         & 0.06           & NA            \\ \hline
    \end{tabularx}
    \label{loss_stats}
\end{table}

\subsubsubsection{Qualitative results}
Out of the 33 respondents, the survey showed that an impressive 93.8\% of the respondents were able to correctly identify that the input image and the synthesized face image belonged to the same  ($score >= 3$). Of those, 14\% were evaluated as being very similar with scores of 5, and 49.3\% were evaluated with scores of 4. %Likert scale provides a way for participants to express their opinions on a spectrum of agreement, allowing us to gather more nuanced feedback on the quality and realism of our avatar reconstruction system. 

\includesmallfigure[The survey result.]{images/head_similarity.png}

\subsubsection{Customizable facial emotions}

\subsubsubsection{Quantitative results}
We use the FaceScape scanned mesh dataset as the ground truth for the evaluation. The comparison is made between the ground truth and the reconstructed mesh. We compare the ground truth with 3 types of face reconstruction model: One use the DECA model with the emotion set to neutral, one use DECA with auto-detected emotion. One use the DECA model with our emotion model. The comparison is additionally made with two views using different input images: one from the front view and one from the side view. The results are shown in the table below.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{Comparison of our emotion model with others}
    \begin{tabular}{|l|ccc|ccc|}
        \hline
        \multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{3}{c|}{\textbf{Front view image input}} & \multicolumn{3}{c|}{\textbf{Side view image input}}                                                                                                           \\ \cline{2-7}
        \multicolumn{1}{|c|}{}                                 & \multicolumn{1}{c|}{\textbf{Mean}}                   & \multicolumn{1}{c|}{\textbf{Median}}                & \textbf{Std} & \multicolumn{1}{c|}{\textbf{Mean}} & \multicolumn{1}{c|}{\textbf{Median}} & \textbf{Std} \\ \hline
        \textbf{DECA + neutral emotion}                        & \multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{}                               &              & \multicolumn{1}{c|}{}              & \multicolumn{1}{c|}{}                &              \\ \hline
        \textbf{DECA + detected emotion}                       & \multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{}                               &              & \multicolumn{1}{c|}{}              & \multicolumn{1}{c|}{}                &              \\ \hline
        \textbf{DECA + our model}                              & \multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{}                               &              & \multicolumn{1}{c|}{}              & \multicolumn{1}{c|}{}                &              \\ \hline
    \end{tabular}
    \label{now_results}
\end{table}


\subsection{Screenshots of the system}