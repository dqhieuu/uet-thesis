\section{Results and discussion}\label{sec:results}

% \subsection{Screenshots of the system}

\subsection{Data Description}
\subsubsection{For avatar reconstruction}

\subsubsubsection*{The HairNet dataset}
We used the public HairNet dataset \cite{zhouHairNetSingleViewHair2018} to train the hair reconstruction model. Using our generation method, we were able to create a dataset of roughly 30,000 images for training the hair reconstruction model from the database of 343 hair models.

\includefigure[Rendered interpolated hairstyles in the HairNet dataset]{images/hairnet_interpolation.jpg}[1.0]

\subsubsubsection*{The StyleGAN dataset}
\label{sec:results/stylegan-dataset}
To evaluate the whole system, we used face images generated from a StyleGAN \cite{karrasStyleBasedGeneratorArchitecture2019} model. These images are diverse in ethnicity, gender, age, and other attributes, making them suitable for evaluating the realism and accuracy of our system. Online tools such as \url{https://thispersondoesnotexist.com/} can be used to generate these images.

\includefigure[Sample images in the StyleGAN dataset.]{images/thispersondoesnotexist.jpg}[1.0]

\subsubsection{For customizable facial emotions}
\subsubsubsection*{The VKIST dataset}
To train the emotion network, we used the VKIST dataset, which consists of 889 images of 127 subjects with 7 captured emotions (neutral, anger, disgust, fear, happiness, sadness, surprise). All images were captured from the front view with a resolution of $2976\times1984$. The link to download the dataset is available at (\url{https://tinyurl.com/vkist-face-front-02}).

90\% of the images were used for training and 10\% for testing. From the dataset, we extracted the face region and resized it to $256\times256$. We then used the pre-trained DECA model to extract the expression parameters from the face images. These expression parameters served as the ground truth for the emotion model.

\includefigure[Sample images in the VKIST dataset.]{images/vkist_dataset.jpg}[1.0]

\subsubsubsection*{The FaceScape dataset}
To evaluate the emotion model, we used the FaceScape dataset, which consists of 847 subjects. The dataset includes scanned 3D face models of 20 different expressions, and 56 images corresponding to 56 camera angles to create each of the scanned models. Only the data of 359 subjects was used because the remaining subjects weren't provided with the captured images/expressions, which made it more difficult to evaluate the effectiveness of the method.

\includefigure[Sample images of a subject in the FaceScape dataset.]{images/facescape_sample_subject_images.jpg}

\subsection{Experimental Scenarios}
To evaluate the accuracy of our system, we conducted two experiments. The first experiment was to evaluate the quality of the avatar generated by our system. The second experiment was to evaluate the quality of the customizable facial emotions.

In the first experiment, we evaluated the quality of the avatar generated by our system by comparing it with the original input image. We used the images generated by the StyleGAN model (elaborated in section \ref{sec:results/stylegan-dataset}) as the input images. We then used the HairNet-based model to reconstruct the hair and the DECA-based model to reconstruct the face. After that, we compared the reconstructed avatar with the original input image using several metrics which are elaborated in section \ref{sec:result/avatar-reconstruction}. We also surveyed to evaluate the similarity between the original input image and the reconstructed avatar using a Likert scale with a 5-point rating system (1-5) for qualitative evaluation.

In the second experiment, we evaluated the quality of the customizable facial emotions by comparing the reconstructed face model with the ground truth. We used the FaceScape dataset (elaborated in section \ref{sec:results/stylegan-dataset}) as the ground truth. The DECA-based model was used to reconstruct the face with the emotion set to neutral and with the emotion set to the emotion predicted by the emotion model. Using the method from the NoW challenge \cite{RingNet:CVPR:2019}, each reconstructed face model was compared with the ground truth model.


\subsection{Evaluation Methods}
\subsubsection{Avatar reconstruction}
\label{sec:result/avatar-reconstruction}
To evaluate the quality of the avatar generated by our system, we have designed a qualitative survey using a Likert scale with a 5-point rating system (1-5). The survey question asked respondents to rate the similarity between the original image input --- a person's portrait --- and the reconstructed avatar output captured as a 2D image. This question served as a metric for assessing the effectiveness of our proposal. Using this question, we aimed to assess how closely the reconstructed avatar output looked compared with the original image input from the perspective of the survey participants.

\subsubsubsection*{Measurements: DFID}
The \glsxtrshort{dfid} \cite{zhuangMoFaNeRFMorphableFacial2022} evaluation method determines whether two portrait images belong to the same person by comparing the difference between their embedding vectors using the Facenet model. Similar to FaceNet \cite{schroffFaceNetUnifiedEmbedding2015}, we determine that two portrait images belong to the same person if their DFID score is less than 1.5.

\subsubsubsection*{Measurements: PSNR, SSIM, LPIPS}
\glsxtrshort{psnr} and \glsxtrshort{ssim} are widely used non-deep learning methods that measure similarity based on specific image attributes and provide information about the similarity in terms of noise and structure.

\textbf{PSNR:} Given a noise-free m√ón monochrome image I and its noisy approximation K, \glsxtrshort{mse} is defined as:

\begin{equation}
    MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2
\end{equation}

The \glsxtrshort{psnr} is defined as

\begin{equation}
    PSNR = 10\cdot\log_{10}\left(\frac{MAX_I^2}{MSE}\right)
\end{equation}
where $MAX_{I}$ is the maximum possible pixel value of the image.

\textbf{SSIM:} The SSIM index is calculated on various windows of an image. The measure between two windows $x$ and $y$ of common size $N \times N$ is:

\begin{equation}
    SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy}+c_2)}{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2+\sigma_y^2+c_2)}
\end{equation}
where $\mu_x$ is the average of $x$, $\mu_y$ is the average of $y$, $\sigma_x^2$ is the variance of $x$, $\sigma_y^2$ is the variance of $y$, and $\sigma_{xy}$ is the covariance of $x$ and $y$. $c_1$ and $c_2$ are two variables to stabilize the division with a weak denominator.


\textbf{LPIPS:} \glsxtrshort{lpips} \cite{zhangUnreasonableEffectivenessDeep2018} is a deep learning-based metric that employs a neural network to learn image features and compute the similarity between two images based on these features.

\subsubsection{Customizable facial emotions}
To evaluate the quality of the customization of the facial emotions, we followed the method used in the NoW challenge and compared the results of different method outputs with the ground truth. The ground truth was the scanned meshes of the FaceScape dataset and was used to compare with the reconstructed face models.

\subsubsubsection*{Measurements: The NoW challenge}
The NoW (Not quite in-the-Wild) challenge \cite{RingNet:CVPR:2019} provides a benchmark method specialized in measuring the accuracy and robustness of 3D face reconstruction methods. It takes 4 inputs: the ground truth mesh, the predicted mesh, the ground truth landmark points, and the predicted landmark points.

It uses a set of 7 landmark points to rigidly align the predicted mesh with the ground truth mesh. The landmark points are the leftmost and the rightmost points of the two eyes, the tip of the nose, the leftmost point, and the rightmost point of the mouth. The error is then calculated using the absolute distance between each scan vertex and the closest point in the mesh surface. The error is output as a vector of error values, in millimeter units, for each vertex in the mesh. The average, median, and standard deviation of the error vector are then calculated and output as the final result.

\includefigure[The landmark points used in the NoW challenge.]{images/now_landmarks.png}[0.6]

\includefigure[The area used for evaluation in the NoW challenge.]{images/now_dataset.png}


\subsection{Experimental Results and Commentary}
\subsubsection{Avatar reconstruction}

\subsubsubsection*{Quantitative results}
Table \ref{loss_stats} illustrates the results of the aforementioned measurements when comparing our method with several other 3D face reconstruction methods. The results show that, in terms of comparison, our results are not as good as many other methods such as i3DMM and MoFaNeRF because they applied the measurements to hairless faces. Our method's PSNR result (lower is better) is better than that of i3DMM and MoFaNeRF. SSIM (higher is better) is worse than that of i3DMM and MoFaNeRF. LPIPS (lower is better) is better than that of i3DMM and worse than that of MoFaNeRF. However, we also see that all of the measurement results meet the accuracy requirements. In particular, the average value of \glsxtrshort{dfid} is 0.25, which indicates that the synthesized output image has been evaluated as retaining the represented features of the same person as the input image.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{Comparison of our system and others for avatar reconstruction}
    \begin{tabularx}{\linewidth}{| X | X | X | X | X |}
        \hline
                            & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} & \textbf{DFID} \\ \hline\hline
        \textbf{Our system} & 23.15         & 0.835         & 0.09           & 0.25          \\ \hline %0.2521 
        \textbf{i3DMM}      & 24.45         & 0.904         & 0.11           & NA            \\ \hline
        \textbf{MoFaNeRF}   & 31.49         & 0.951         & 0.06           & NA            \\ \hline
    \end{tabularx}
    \label{loss_stats}
\end{table}

\subsubsubsection*{Qualitative results}
Out of the 33 respondents, the survey showed that an impressive 93.8\% of the respondents rated that the input image and the synthesized face of the reconstructed 3D model image looked similar ($\textrm{score} \geq 3$). 14\% of the respondents rated the synthesized face images as very similar to the original image ($\textrm{score} = 5$), and 49.3\% rated with scores of 4. %Likert scale provides a way for participants to express their opinions on a spectrum of agreement, allowing us to gather more nuanced feedback on the quality and realism of our avatar reconstruction system. 

\includefigure[The avatar reconstruction survey result.]{images/head_similarity.png}[0.6]

\subsubsection{Customizable facial emotions}

\subsubsubsection*{Quantitative results}
We used the FaceScape scanned mesh dataset as the ground truth for the evaluation. The comparison was made between the ground truth --- the FaceScape mesh --- and the reconstructed mesh. The benchmarking method is the NoW challenge \cite{RingNet:CVPR:2019}, which is one of the most widely used methods for evaluating 3D face reconstruction methods.

We compared the ground truth with 2 types of face reconstruction models: One uses the DECA model with the emotion set to neutral, one uses DECA with auto-detected emotion, and one uses the DECA model with our emotion model. The reason for using the DECA model with the emotion set to neutral is to compare the results reconstructed using only the shape parameters (\beta) with the results reconstructed using both the shape parameters (\beta) and the expression parameters (\psi). A good result would be that the results reconstructed using both the shape parameters (\beta) and the expression parameters (\psi) are better than the results reconstructed using only the shape parameters (\beta).

The comparison was additionally made with two views using different input images: one from the front view and one from the side view. The benchmark is done on 20 subjects with 4 different expressions (neutral, smile, sadness, anger), and using 2 different views which results in 160 data points. The evaluated data can be downloaded at (\url{https://tinyurl.com/now-eval-emotion}). The results are shown in the table below.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{NoW challenge error results of face reconstruction methods}
    {\renewcommand{\arraystretch}{1.3}
        \begin{tabularx}{\linewidth}{X|ccc|ccc}
            \hline
            \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}}                                         & \multicolumn{3}{c|}{\textbf{Front-view image input}} & \multicolumn{3}{c}{\textbf{Side-view image input}}                                                                 \\ \cline{2-7}
            \multicolumn{1}{c|}{}                                                                         & \textbf{Mean}                                        & \textbf{Median}                                    & \textbf{Std} & \textbf{Mean} & \textbf{Median} & \textbf{Std} \\ \hline
            \textbf{DECA + neutral emotion}                                                               & 1.83                                                 & 1.46                                               & 1.49         & 1.77          & 1.40            & 1.44         \\
            % \textbf{DECA + detected emotion}                                                              &                 &                   &                &                 &                  &               \\
            \textbf{\begin{tabular}[c]{@{}l@{}}DECA + customized emotion \\ using our model\end{tabular}} & 1.95                                                 & 1.55                                               & 1.61         & 1.88          & 1.50            & 1.56         \\ \hline
        \end{tabularx}
    }
    \label{now_results}
\end{table}

\includefigure[NoW challenge error by percentage of face reconstruction methods.]{images/now_challenge_emotion.png}

From the results, we can see that the results reconstructed using both the shape parameters (\beta) and the expression parameters (\psi) are slightly worse than the results reconstructed using only the shape parameters (\beta). This is probably because the expression parameters (\psi) are not accurate enough to be used for reconstructing the face. Another reason is that different facial expressions don't make enough impact for the NoW metrics to reflect the changes in values. Nevertheless, the results are still acceptable because the difference is not significant.

\includefigure[Emotions generated by our emotion customization model.]{images/emotion_net_demo.png}