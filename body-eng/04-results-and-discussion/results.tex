\section{Results and discussion}\label{sec:results}

\subsection{Screenshots of the system}

\subsection{Data Description}
\subsubsection{For avatar reconstruction}

\subsubsubsection{The HairNet dataset}
We used the public HairNet dataset \cite{zhouHairNetSingleViewHair2018} to train the hair reconstruction model. Using our generation method, we were able to create a dataset of roughly 30,000 images for training the hair reconstruction model from the database of 343 hair models.

\includefigure[Rendered interpolated hairstyles in the HairNet dataset]{images/hairnet_interpolation.jpg}[1.0]

\subsubsubsection{The StyleGAN dataset}
\label{sec:results/stylegan-dataset}
To evaluate the whole system, we used face images generated from a StyleGAN \cite{karrasStyleBasedGeneratorArchitecture2019} model. These images are diverse in ethnicity, gender, age, and other attributes, making them suitable for evaluating the realism and accuracy of our system. Online tools such as \url{https://thispersondoesnotexist.com/} can be used to generate these images.

\includefigure[Sample images in the StyleGAN dataset.]{images/thispersondoesnotexist.jpg}[1.0]

\subsubsection{For customizable facial emotions}
\subsubsubsection{The VKIST dataset}
To train the emotion network, we use the VKIST dataset which consists of 889 images of 127 subjects with 7 captured emotions (neutral, anger, disgust, fear, happiness, sadness, surprise). All images are captured from the front view with a resolution of $2976\times1984$. The link to download the dataset is available at (\url{https://tinyurl.com/vkist-face-front-02}).

90\% of the images are used for training and 10\% for testing. From the dataset, we extract the face region and resize it to $256\times256$. We then use the pre-trained DECA model to extract the expression parameters from the face images. These expression parameters serve as the ground truth for the emotion model.

\includefigure[Sample images in the VKIST dataset.]{images/vkist_dataset.jpg}[1.0]

\subsubsubsection{The FaceScape dataset}
To evaluate the emotion model, we used the FaceScape dataset, which consists of 847 subjects, is used. The dataset consists of scanned 3D face models of 20 different expressions, and 56 images corresponding to 56 camera angles to create each of the scanned models. Only the data of 359 subjects was used because the remaining subjects weren't provided with the captured images/expressions, which makes it more difficult to evaluate the effectiveness of the method.

\includefigure[Sample images of a subject in the FaceScape dataset.]{images/facescape_sample_subject_images.jpg}

\subsection{Experimental Scenarios}
To evaluate the accuracy of our system, we conducted two experiments. The first experiment is to evaluate the quality of the avatar generated by our system. The second experiment is to evaluate the quality of the customizable facial emotions.

In the first experiment, we evaluate the quality of the avatar generated by our system by comparing it with the original input image. We use the images generated by the StyleGAN model (explained in section \ref{sec:results/stylegan-dataset}) as the input images. We then used the HairNet-based model to reconstruct the hair and the DECA-based model to reconstruct the face. We then compared the reconstructed avatar with the original input image using several metrics which are elaborated in section \ref{sec:result/avatar-reconstruction}. We also surveyed to evaluate the similarity between the original input image and the reconstructed avatar using a Likert scale with a 5-point rating system (1-5) for qualitative evaluation.

In the second experiment, we evaluate the quality of the customizable facial emotions by comparing the reconstructed face model with the ground truth. We used the FaceScape dataset (explained in section \ref{sec:results/stylegan-dataset}) as the ground truth. We then used the DECA-based model to reconstruct the face with the emotion set to neutral and with the emotion set to the emotion predicted by the emotion model. We then compared the reconstructed face model with the ground truth using the method used in the NoW challenge \cite{RingNet:CVPR:2019}.


\subsection{Evaluation Methods}
\subsubsection{Avatar reconstruction}
\label{sec:result/avatar-reconstruction}
To evaluate the quality of the avatar generated by our system, we have designed a qualitative survey using a Likert scale with a 5-point rating system (1-5). The survey question asks respondents to rate the degree of similarity between the original input and the avatar output. This question serves as a key metric for assessing the effectiveness of our proposal.
Using this question, we aimed to assess how closely the avatar output resembles the original input from the perspective of the survey participants.
\subsubsubsection{Measurements: DFID}
The \glsxtrshort{dfid} \cite{zhuangMoFaNeRFMorphableFacial2022} evaluation method determines whether two portrait images belong to the same person by comparing the difference between their embedding vectors using the Facenet model. Similar to FaceNet \cite{schroffFaceNetUnifiedEmbedding2015}, we determine that two portrait images belong to the same person if their DFID score is less than 1.5.

\subsubsubsection{Measurements: PSNR, SSIM, LPIPS}
\glsxtrshort{psnr} and \glsxtrshort{ssim} are widely used non-deep learning methods that measure similarity based on specific image attributes and provide information about the similarity in terms of noise and structure.

\textbf{PSNR:} Given a noise-free mÃ—n monochrome image I and its noisy approximation K, \glsxtrshort{mse} is defined as:

\begin{equation}
    MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2
\end{equation}

The \glsxtrshort{psnr} is defined as

\begin{equation}
    PSNR = 10\cdot\log_{10}\left(\frac{MAX_I^2}{MSE}\right)
\end{equation}
where $MAX_{I}$ is the maximum possible pixel value of the image.

\textbf{SSIM:} The SSIM index is calculated on various windows of an image. The measure between two windows $x$ and $y$ of common size $N \times N$ is:

\begin{equation}
    SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy}+c_2)}{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2+\sigma_y^2+c_2)}
\end{equation}
where $\mu_x$ is the average of $x$, $\mu_y$ is the average of $y$, $\sigma_x^2$ is the variance of $x$, $\sigma_y^2$ is the variance of $y$, and $\sigma_{xy}$ is the covariance of $x$ and $y$. $c_1$ and $c_2$ are two variables to stabilize the division with a weak denominator.


\textbf{LPIPS:} \glsxtrshort{lpips} \cite{zhangUnreasonableEffectivenessDeep2018} is a deep learning-based metric that employs a neural network to learn image features and compute the similarity between two images based on these features.

\subsubsection{Customizable facial emotions}
To evaluate the quality of the customization of the facial emotions, we follow the method used in the NoW challenge and compare the results of different method outputs with the ground truth. The ground truth is the scanned meshes of the FaceScape dataset, compared with the reconstructed face models.

\subsubsubsection{Measurements: The NoW challenge}
The NoW (Not quite in-the-Wild) challenge \cite{RingNet:CVPR:2019} provides a benchmark method specialized in measuring the accuracy and robustness of 3D face reconstruction methods. It takes 4 inputs: the ground truth mesh, the predicted mesh, the ground truth landmark points, and the predicted landmark points.

It uses a set of 7 landmark points to rigidly align the predicted mesh with the ground truth mesh. The landmark points are the leftmost and the rightmost points of the two eyes, the tip of the nose, the leftmost point, and the rightmost point of the mouth. The error is then calculated using the absolute distance between each scan vertex and the closest point in the mesh surface. The error is output as a vector of error values, in milliter units, for each vertex in the mesh. The average, median, and standard deviation of the error vector are then calculated and output as the final result.

\includefigure[The landmark points used in the NoW challenge.]{images/now_landmarks.png}[0.6]

\includefigure[The area used for evaluation in the NoW challenge.]{images/now_dataset.png}


\subsection{Experimental Results and Commentary}
\subsubsection{Avatar reconstruction}

\subsubsubsection{Quantitative results}
Table \ref{loss_stats} illustrates the results of the aforementioned measurements when comparing our method with several other 3D face reconstruction methods. The results show that, in terms of comparison, our results are not as good as many other methods such as i3DMM and MoFaNeRF because they applied the measurements to hairless faces. However, we also see that all of the measurement results meet the requirements. In particular, the average value of \glsxtrshort{dfid} is 0.25, which indicates that the synthesized output image has been evaluated as retaining the represented features of the same person as the input image.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{Comparison of our proposal and others}
    \begin{tabularx}{\linewidth}{| X | X | X | X | X |}
        \hline
                            & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} & \textbf{DFID} \\ \hline\hline
        \textbf{Our system} & 23.15         & 0.835         & 0.09           & 0.25          \\ \hline %0.2521 
        \textbf{i3DMM}      & 24.45         & 0.904         & 0.11           & NA            \\ \hline
        \textbf{MoFaNeRF}   & 31.49         & 0.951         & 0.06           & NA            \\ \hline
    \end{tabularx}
    \label{loss_stats}
\end{table}

\subsubsubsection{Qualitative results}
Out of the 33 respondents, the survey showed that an impressive 93.8\% of the respondents were able to correctly identify that the input image and the synthesized face image belonged to the same  ($score >= 3$). Of those, 14\% were evaluated as being very similar with scores of 5, and 49.3\% were evaluated with scores of 4. %Likert scale provides a way for participants to express their opinions on a spectrum of agreement, allowing us to gather more nuanced feedback on the quality and realism of our avatar reconstruction system. 

\includefigure[The survey result.]{images/head_similarity.png}[0.6]

\subsubsection{Customizable facial emotions}

\subsubsubsection{Quantitative results}
We use the FaceScape scanned mesh dataset as the ground truth for the evaluation. The comparison is made between the ground truth and the reconstructed mesh. We compare the ground truth with 3 types of face reconstruction models: One uses the DECA model with the emotion set to neutral, and one uses DECA with auto-detected emotion. One uses the DECA model with our emotion model. The comparison is additionally made with two views using different input images: one from the front view and one from the side view. The results are shown in the table below. The benchmarking method is the NoW challenge \cite{RingNet:CVPR:2019}, which is the most widely used method for evaluating 3D face reconstruction methods.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{Comparison of our emotion model with others}
    \begin{tabular}{|l|ccc|ccc|}
        \hline
        \multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{3}{c|}{\textbf{Front view image input}} & \multicolumn{3}{c|}{\textbf{Side view image input}}                                                                                                           \\ \cline{2-7}
        \multicolumn{1}{|c|}{}                                 & \multicolumn{1}{c|}{\textbf{Mean}}                   & \multicolumn{1}{c|}{\textbf{Median}}                & \textbf{Std} & \multicolumn{1}{c|}{\textbf{Mean}} & \multicolumn{1}{c|}{\textbf{Median}} & \textbf{Std} \\ \hline
        \textbf{DECA + neutral emotion}                        & \multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{}                               &              & \multicolumn{1}{c|}{}              & \multicolumn{1}{c|}{}                &              \\ \hline
        \textbf{DECA + detected emotion}                       & \multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{}                               &              & \multicolumn{1}{c|}{}              & \multicolumn{1}{c|}{}                &              \\ \hline
        \textbf{DECA + our model}                              & \multicolumn{1}{c|}{}                                & \multicolumn{1}{c|}{}                               &              & \multicolumn{1}{c|}{}              & \multicolumn{1}{c|}{}                &              \\ \hline
    \end{tabular}
    \label{now_results}
\end{table}
