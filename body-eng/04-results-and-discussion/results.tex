\section{Results and discussion}
\label{sec:results}

\subsection{Data Description}
\subsubsection{Hair reconstruction}
\subsubsubsection{Training}
To train the hair reconstruction model, we used the public HairNet dataset \cite{zhouHairNetSingleViewHair2018}. Using our generation method, we were able to create a dataset of roughly 30,000 images for training the hair reconstruction model from the database of 343 hair models.

\subsubsection{Face reconstruction}
\subsubsubsection{Training}
We use the pre-trained DECA model for face reconstruction. The system can swap DECA with other face reconstruction models such as MICA \cite{zielonkaMetricalReconstructionHuman2022} or EMOCA. We then apply the base face reconstruction model with our emotion model to generate the final avatar model.

\subsubsection{Avatar reconstruction}
Avatar reconstruction is the combination of hair reconstruction and face reconstruction. We use the pre-trained DECA (or MICA, EMOCA) model for face reconstruction. We then apply the base face reconstruction model with our emotion model to generate the final avatar model.

\subsubsubsection{Evaluation}
To evaluate the system, we used face images generated from a StyleGAN \cite{karrasStyleBasedGeneratorArchitecture2019} model. These images are diverse in ethnicity, gender, age, and other attributes, making them suitable for evaluating the realism and accuracy of our system.

\subsubsection{Customizable facial emotions}
\subsubsubsection{Training}
To train the emotion network, we use the VKIST dataset which consists of 889 images of 127 subjects with 7 captured emotions (neutral, anger, disgust, fear, happiness, sadness, surprise). All images are captured from the front view with a resolution of 2976x1984. 90\% of the images are used for training and 10\% for testing. From the dataset, we extract the face region and resize it to 256x256. We then use the pre-trained DECA model to extract the expression parameters from the face images. These expression parameters serve as the ground truth for the emotion model.

\subsubsubsection{Evaluation}
To evaluate the emotion model, the FaceScape dataset, which consists of 847 subjects, is used. The dataset consists of scanned 3D face models of 20 different expressions, and 56 images corresponding to 56 camera angles. However, only the data of 359 subjects is used because the remaining subjects aren't provided with the captured images/expressions.

\subsection{Experimental Scenarios}


\subsection{Evaluation Methods}
\subsubsection{Avatar reconstruction}
To evaluate the quality of the avatar generated by our system, we have designed a qualitative survey using a Likert scale with a 5-point rating system (1-5). The survey question asks respondents to rate the degree of similarity between the original input and the avatar output. This question serves as a key metric for assessing the effectiveness of our proposal.
Using this question, we aimed to assess how closely the avatar output resembles the original input from the perspective of the survey participants.
\subsubsubsection{Measurements: DIFD}
The DIFD (Difference in Facial Descriptors) evaluation method determines whether two portrait images belong to the same person by comparing the difference between their embedding vectors using the Facenet model. Similar to FaceNet, we determine that two portrait images belong to the same person if their DIFD score is less than 1.5.

\subsubsubsection{Measurements: PSNR, SSIM, LPIPS}
PSNR and SSIM are widely used non-deep learning methods that measure similarity based on specific image attributes and provide information about the similarity in terms of noise and structure.
LPIPS is a deep learning-based metric that employs a neural network to learn image features and compute the similarity between two images based on these features.

\subsubsection{Customizable emotions}
To evaluate the quality of the customization of the facial emotions, we follow the method used in the NoW challenge.
\subsubsubsection{Measurements: The NoW challenge}
The NoW challenge provides a benchmark method specialized in face reconstruction comparison.


\subsection{Experimental Results and Commentary}
\subsubsection{Quantitative results}
Table \ref{loss_stats} illustrates the results of the aforementioned measurements when comparing our method with several other 3D face reconstruction methods. The results show that, in terms of comparison, our results are not as good as many other methods such as i3DMM and MoFaNeRF because they applied the measurements to hairless faces. However, we also see that all of the measurement results meet the requirements. In particular, the average value of DIFD is 0.25, which indicates that the synthesized output image has been evaluated as retaining the represented features of the same person as the input image.

\begin{table}[H]
    \centering
    \captionsetup{font=bf}
    \caption{Comparison of our proposal and others}
    \begin{tabularx}{\linewidth}{| X | X | X | X | X |}
        \hline
                            & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} & \textbf{DIFD} \\ \hline\hline
        \textbf{Our system} & 23.15         & 0.835         & 0.09           & 0.25          \\ \hline %0.2521 
        \textbf{i3DMM}      & 24.45         & 0.904         & 0.11           & NA            \\ \hline
        \textbf{MoFaNeRF}   & 31.49         & 0.951         & 0.06           & NA            \\ \hline
    \end{tabularx}
    \label{loss_stats}
\end{table}

\subsubsection{Qualitative results}
Out of the 33 respondents, the survey showed that an impressive 93.8\% of the respondents were able to correctly identify that the input image and the synthesized face image belonged to the same  ($score >= 3$). Of those, 14\% were evaluated as being very similar with scores of 5, and 49.3\% were evaluated with scores of 4. %Likert scale provides a way for participants to express their opinions on a spectrum of agreement, allowing us to gather more nuanced feedback on the quality and realism of our avatar reconstruction system. 

\includesmallfigure[The survey result.]{images/head_similarity.png}